{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f4f35e3b",
      "metadata": {
        "id": "f4f35e3b"
      },
      "source": [
        "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel $\\rightarrow$ Restart) and then **run all cells** (in the menubar, select Cell $\\rightarrow$ Run All).\n",
        "\n",
        "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3ef50ca1",
      "metadata": {
        "id": "3ef50ca1"
      },
      "outputs": [],
      "source": [
        "NAME = \"MAYANK DAHIYA\"\n",
        "COLLABORATORS = \"W3 Schools , GeekforGeeks , OpenML , Google\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "762abab6",
      "metadata": {
        "id": "762abab6"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8753048a-08d4-4a2e-b750-50e923c15f22",
      "metadata": {
        "id": "8753048a-08d4-4a2e-b750-50e923c15f22"
      },
      "source": [
        "## CSL2050: Pattern Recognition and Machine Learning<br>\n",
        "Programming Assignment-2<br>\n",
        "Spring 2025<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6e7297f-a2a2-4927-8501-35db45807ac8",
      "metadata": {
        "id": "f6e7297f-a2a2-4927-8501-35db45807ac8"
      },
      "source": [
        "## Linear Regression\n",
        "In this assignment, we will explore linear regression by implementing fundamental modules such as Mean Squared Error (MSE), Gradient Descent, and Prediction. These building blocks will then be utilized to experiment with a real-world dataset for solving a linear regression problem. Please ensure that you solve the problems sequentially for a structured learning experience."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9969ec7-b433-47a9-a564-d9a99a513598",
      "metadata": {
        "id": "b9969ec7-b433-47a9-a564-d9a99a513598"
      },
      "source": [
        "**Problem-2.01:** Write a function mean_squared_error(y_true, y_pred) that calculates the Mean Squared Error (MSE) between the true values y_true and predicted values y_pred."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ce9b1d59-496d-4cea-9159-3db30701fed5",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9724d3c94bc4f006ca6bd3d634a687fa",
          "grade": false,
          "grade_id": "mean_squared_error",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "ce9b1d59-496d-4cea-9159-3db30701fed5"
      },
      "outputs": [],
      "source": [
        "def mean_squared_error(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate the Mean Squared Error between true and predicted values.\n",
        "\n",
        "    Args:\n",
        "        y_true (numpy.ndarray): True values of shape (n_samples,).\n",
        "        y_pred (numpy.ndarray): Predicted values of shape (n_samples,).\n",
        "\n",
        "    Returns:\n",
        "        float: Mean Squared Error.\n",
        "    \"\"\"\n",
        "\n",
        "    squared_differences = (y_true - y_pred)**2  # Calculate squared differences\n",
        "    mse = np.mean(squared_differences)  # Calculate mean of squared differences\n",
        "\n",
        "    return mse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "406cecb0-59dd-4a44-a0a2-58b5f6e02549",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5b3a7be20581874715c66eb8487fec74",
          "grade": true,
          "grade_id": "mean_squared_error_test",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "406cecb0-59dd-4a44-a0a2-58b5f6e02549"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Visible Test Case\n",
        "import numpy as np\n",
        "y_true = np.array([1.0, 2.0, 3.0])\n",
        "y_pred = np.array([1.0, 2.0, 3.0])\n",
        "assert mean_squared_error(y_true, y_pred) == 0, \"Test Case 1 Failed\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "696c5ec4-4583-4b59-bc97-0e7ea81be768",
      "metadata": {
        "id": "696c5ec4-4583-4b59-bc97-0e7ea81be768"
      },
      "source": [
        "**Problem-2.02:** Write a function ols_coefficients(X, y) that calculates the coefficients for a simple linear regression problem using the Ordinary Least Squares (OLS) formula. Note: Coefficient in hyperplane \\theta_0+\\theta_1x_1+\\theta_2x_2 are \\theta_0, \\theta_1 and \\theta_2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "12b1fadd-3545-4c29-a082-49238f418285",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2db5c514b1dc664de9001f41cf424a5a",
          "grade": false,
          "grade_id": "ols_coefficients",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "12b1fadd-3545-4c29-a082-49238f418285"
      },
      "outputs": [],
      "source": [
        "def ols_coefficients(X, y):\n",
        "    \"\"\"\n",
        "    Calculate the OLS coefficients for linear regression.\n",
        "\n",
        "    Args:\n",
        "        X (numpy.ndarray): Feature matrix of shape (n_samples, n_features).\n",
        "        y (numpy.ndarray): Target vector of shape (n_samples,).\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Coefficients of shape (n_features + 1,).\n",
        "                       The first value is the intercept, and the rest are feature coefficients.\n",
        "    \"\"\"\n",
        "    X = np.c_[np.ones(X.shape[0]), X]   # Add a column to X for the intercept\n",
        "\n",
        "    coefficients = np.linalg.solve(X.T @ X, X.T @ y) # Calculate coefficients using OLS formula\n",
        "\n",
        "    return coefficients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "eab71fea-2dc4-4900-8ddd-c8dcb8471717",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ef8d1339b318e0102592a84b667a9a01",
          "grade": true,
          "grade_id": "ols_coefficients_test",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "eab71fea-2dc4-4900-8ddd-c8dcb8471717",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c930054-2ec0-4b67-99f4-d66f0c52c5e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (8.3.4)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pytest) (24.2)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest) (1.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytest\n",
        "import pytest\n",
        "# Test case 1: Single feature, no intercept\n",
        "def test_case_1():\n",
        "    X = np.array([[1], [2], [3]])\n",
        "    y = np.array([2, 4, 6])\n",
        "    expected = np.array([0., 2.])  # y = 2x\n",
        "    np.testing.assert_almost_equal(ols_coefficients(X, y), expected, decimal=6)\n",
        "\n",
        "# Test case 2: Single feature, with intercept\n",
        "def test_case_2():\n",
        "    X = np.array([[1], [2], [3]])\n",
        "    y = np.array([3, 5, 7])\n",
        "    expected = np.array([1., 2.])  # y = 1 + 2x\n",
        "    np.testing.assert_almost_equal(ols_coefficients(X, y), expected, decimal=6)\n",
        "\n",
        "test_case_1()\n",
        "test_case_2()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db79e214-d953-48f7-afda-deffb13fb729",
      "metadata": {
        "id": "db79e214-d953-48f7-afda-deffb13fb729"
      },
      "source": [
        "**Problem-2.03:** Implement a function gradient_descent(X, y, lr, epochs) to perform gradient descent for linear regression.\n",
        "\n",
        "Given a feature matrix X of shape (n_samples,n_features) a target vector y of shape (n_samples,), a learning rate lr, and the number of iterations epochsepochs, your task is to iteratively update the coefficients θ to minimize the mean squared error between the predicted and actual target values.\n",
        "\n",
        "The function should return the optimized coefficients θ, which are of shape (n_features+1,). (Note the line/hyperplan has equations like θ_0 + θ_1x_1 + θ_2x_2 + ....+θ_nx_n=0.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ff146ab6-6872-4032-99aa-3d78871b3c3b",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "497d25fe66401c5238b24cd6458eae81",
          "grade": false,
          "grade_id": "gradient_descent",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "ff146ab6-6872-4032-99aa-3d78871b3c3b"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(X, y, lr, epochs):\n",
        "    \"\"\"\n",
        "    Perform gradient descent for linear regression.\n",
        "\n",
        "    Args:\n",
        "        X (numpy.ndarray): Feature matrix of shape (n_samples, n_features).\n",
        "        y (numpy.ndarray): Target vector of shape (n_samples,).\n",
        "        lr (float): Learning rate.\n",
        "        epochs (int): Number of iterations.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Optimized coefficients of shape (n_features+1).\n",
        "    \"\"\"\n",
        "    n_samples, n_features = X.shape  # Extract no. of rows and column of x.\n",
        "    theta = np.zeros(n_features + 1)  # Initialize vector theta with zeros , +1 for intercept\n",
        "\n",
        "    X = np.c_[np.ones(n_samples), X]  # Add column of ones to X for intercept\n",
        "\n",
        "    for _ in range(epochs):  # Perform gradient descent for the specified number of epochs\n",
        "        predictions = X @ theta  # Calculate predictions\n",
        "\n",
        "        errors = predictions - y  # Calculate errors\n",
        "\n",
        "        # Update coefficients using the gradient\n",
        "        gradient = (X.T @ errors) / n_samples\n",
        "        theta = theta - lr * gradient\n",
        "\n",
        "    return theta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1bb36687-bc18-412d-842a-953db6ad20ed",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f1752e624ded63b40693df419109b115",
          "grade": true,
          "grade_id": "gradient_descent_test",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "1bb36687-bc18-412d-842a-953db6ad20ed"
      },
      "outputs": [],
      "source": [
        "# Test case 1: Simple case with one feature and bias term\n",
        "def test_case_1():\n",
        "    X = np.array([[1], [2], [3]])\n",
        "    y = np.array([2, 4, 6])  # Perfectly linear relationship y = 2x\n",
        "    lr = 0.01\n",
        "    epochs = 1000\n",
        "    theta = gradient_descent(X, y, lr, epochs)\n",
        "    # Assert bias and slope\n",
        "    assert np.abs(theta[0] - 0) < 0.5, f\"term {theta[0]} is not close enough to 0\"\n",
        "    assert np.abs(theta[1] - 2) < 0.5, f\"term {theta[1]} is not close enough to 2\"\n",
        "test_case_1()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce2c72c6-110d-4af0-bf23-146cd4c8b030",
      "metadata": {
        "id": "ce2c72c6-110d-4af0-bf23-146cd4c8b030"
      },
      "source": [
        "**Problem-2.04:** Write a function predict(X, theta) that predicts the target values y for each sample of a given feature matrix X and the learned coefficients θ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1f4009de-260a-4f49-baac-4f6a28330019",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9a14535d976f1a7d6eeec77b7c0c5623",
          "grade": false,
          "grade_id": "predict",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "1f4009de-260a-4f49-baac-4f6a28330019"
      },
      "outputs": [],
      "source": [
        "def predict(X, theta):\n",
        "    \"\"\"\n",
        "    Predict target values for given features and coefficients.\n",
        "\n",
        "    Args:\n",
        "        X (numpy.ndarray): Feature matrix of shape (n_samples, n_features).\n",
        "        theta (numpy.ndarray): Coefficients of shape (n_features,).\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Predicted values of shape (n_samples,).\n",
        "    \"\"\"\n",
        "    # Add column of ones to X for the intercept if it's not already present\n",
        "    if X.shape[1] != theta.shape[0]:\n",
        "        X = np.c_[np.ones(X.shape[0]), X]\n",
        "\n",
        "    # Calculate prediction using dot product of feature and coefficient\n",
        "    predictions = X @ theta\n",
        "\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f8def26a-cd16-42fa-a1dd-b1b9204967a8",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ec6d80e6467c72e8629a5b83f1c2fcf6",
          "grade": true,
          "grade_id": "predict_test",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "f8def26a-cd16-42fa-a1dd-b1b9204967a8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Test case 1: Single feature, single sample\n",
        "def test_case_1():\n",
        "    X = np.array([[2]])  # Single sample with one feature\n",
        "    theta = np.array([1, 2])  # Bias = 1, Coefficient = 2\n",
        "    expected = np.array([5])  # Prediction: 1 + 2*2 = 5\n",
        "    np.testing.assert_almost_equal(predict(X, theta), expected, decimal=6)\n",
        "\n",
        "# Test case 2: Single feature, multiple samples\n",
        "def test_case_2():\n",
        "    X = np.array([[1], [2], [3]])  # Three samples, one feature\n",
        "    theta = np.array([0.5, 1.5])  # Bias = 0.5, Coefficient = 1.5\n",
        "    expected = np.array([2.0, 3.5, 5.0])  # Predictions: 0.5 + 1.5*X\n",
        "    np.testing.assert_almost_equal(predict(X, theta), expected, decimal=6)\n",
        "\n",
        "test_case_1()\n",
        "test_case_2()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89ec0084-c4fe-4407-aae9-719f0fe2bf5f",
      "metadata": {
        "id": "89ec0084-c4fe-4407-aae9-719f0fe2bf5f"
      },
      "source": [
        "**Problem-2.05:** Putting it all together, now we can write a function that leverages the previously implemented functions, including Mean Squared Error (MSE), gradient descent, and prediction, and perform linear regression using gradient descent (assuming MSE loss)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e787219f-0e4a-43c5-ab6b-851381fdf7db",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f56011aa682286909deb6b30358b9729",
          "grade": false,
          "grade_id": "LinearRegressionUsingGD",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "e787219f-0e4a-43c5-ab6b-851381fdf7db"
      },
      "outputs": [],
      "source": [
        "def LinearRegressionUsingGD(X,y,lr, epochs):\n",
        "  theta=gradient_descent(X, y, lr, epochs)\n",
        "  predictions=predict(X,theta)\n",
        "  return theta, predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "74502d48-b6c2-4d49-89e2-fa01a7c09c72",
      "metadata": {
        "id": "74502d48-b6c2-4d49-89e2-fa01a7c09c72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ea28d76-a7a1-464f-98b4-132b7454c13f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [0.11071521 0.95129619]\n",
            "Predictions: [1.0620114  2.01330759 2.96460378]\n"
          ]
        }
      ],
      "source": [
        "# Example Usage\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[1], [2], [3]])\n",
        "y = np.array([1, 2, 3])\n",
        "\n",
        "theta, predictions = LinearRegressionUsingGD(X,y,lr=0.01, epochs=1000)\n",
        "\n",
        "print(\"Coefficients:\", theta)\n",
        "print(\"Predictions:\", predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "88b4e04a-6873-445b-8c5c-75793451eb82",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3204f3d1fc52cda614983ca810c29c0d",
          "grade": true,
          "grade_id": "LinearRegressionUsingGD_test",
          "locked": true,
          "points": 6,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "88b4e04a-6873-445b-8c5c-75793451eb82"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Test Case 1: Simple linear regression with a single feature\n",
        "def test_case_1():\n",
        "    X = np.array([[1], [2], [3]])  # Feature matrix\n",
        "    y = np.array([1, 2, 3])  # Target values\n",
        "\n",
        "    theta, predictions = LinearRegressionUsingGD(X,y,lr=0.01, epochs=1000)\n",
        "\n",
        "    # Calculate residuals\n",
        "    residuals = np.abs(predictions - y)\n",
        "\n",
        "    # Assert that residuals are very small (e.g., less than 0.01)\n",
        "    np.testing.assert_array_less(residuals, np.full_like(residuals, 0.4))\n",
        "\n",
        "test_case_1()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db6155da-f8d5-4122-bbaf-84c173c9ac46",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "67eb9395d181a4c30c4e89cbdc15aa1f",
          "grade": false,
          "grade_id": "realWorldData",
          "locked": true,
          "points": 4,
          "schema_version": 3,
          "solution": false,
          "task": true
        },
        "id": "db6155da-f8d5-4122-bbaf-84c173c9ac46"
      },
      "source": [
        "**Problem-2.06:** Leverage your implementation and choose any real word regression dataset of your choice. Split the data into 80-10-10% of train-val-test. Report train and test MSE of your linear regression model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Loading the Boston House dataset (specifying version 1 to avoid warning)\n",
        "boston = fetch_openml(name=\"boston\", version=1, as_frame=False)\n",
        "X, y = boston.data, boston.target\n",
        "\n",
        "# Displaying the first 5 rows of the dataset\n",
        "boston_df = pd.DataFrame(X, columns=boston.feature_names)\n",
        "boston_df['MEDV'] = y  # Adding target column\n",
        "print(\"First 5 rows of the Boston Housing dataset:\")\n",
        "print(boston_df.head())\n",
        "\n",
        "# Splitting data into 80% train, 10% validation, 10% test\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Normalize features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Gradient Descent Function\n",
        "def gradient_descent(X, y, lr, epochs):\n",
        "    n_samples, n_features = X.shape\n",
        "    theta = np.zeros(n_features + 1)\n",
        "    X = np.c_[np.ones(n_samples), X]\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        predictions = X @ theta\n",
        "        errors = predictions - y\n",
        "        gradient = (X.T @ errors) / n_samples\n",
        "        theta -= lr * gradient\n",
        "\n",
        "    return theta\n",
        "\n",
        "# Predict Function\n",
        "def predict(X, theta):\n",
        "    X = np.c_[np.ones(X.shape[0]), X]\n",
        "    return X @ theta\n",
        "\n",
        "# Mean Squared Error Function\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# Train Linear Regression using Gradient Descent\n",
        "theta = gradient_descent(X_train, y_train, lr=0.01, epochs=1000)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = predict(X_train, theta)\n",
        "y_test_pred = predict(X_test, theta)\n",
        "\n",
        "# Compute and print MSE\n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "\n",
        "print(f\"\\nTrain MSE: {train_mse:.4f}\")\n",
        "print(f\"Test MSE: {test_mse:.4f}\")\n"
      ],
      "metadata": {
        "id": "dkRIQzDNjRFu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fea3197-2e8f-4c14-c7e3-6c1f4465e037"
      },
      "id": "dkRIQzDNjRFu",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of the Boston Housing dataset:\n",
            "      CRIM    ZN INDUS CHAS    NOX     RM   AGE     DIS RAD    TAX PTRATIO  \\\n",
            "0  0.00632  18.0  2.31    0  0.538  6.575  65.2    4.09   1  296.0    15.3   \n",
            "1  0.02731   0.0  7.07    0  0.469  6.421  78.9  4.9671   2  242.0    17.8   \n",
            "2  0.02729   0.0  7.07    0  0.469  7.185  61.1  4.9671   2  242.0    17.8   \n",
            "3  0.03237   0.0  2.18    0  0.458  6.998  45.8  6.0622   3  222.0    18.7   \n",
            "4  0.06905   0.0  2.18    0  0.458  7.147  54.2  6.0622   3  222.0    18.7   \n",
            "\n",
            "        B LSTAT  MEDV  \n",
            "0   396.9  4.98  24.0  \n",
            "1   396.9  9.14  21.6  \n",
            "2  392.83  4.03  34.7  \n",
            "3  394.63  2.94  33.4  \n",
            "4   396.9  5.33  36.2  \n",
            "\n",
            "Train MSE: 21.8602\n",
            "Test MSE: 22.5925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a51931df-3ced-4e0a-90bb-563a67bccc8a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "57cb8b9694569192581c273dde2f8e4a",
          "grade": false,
          "grade_id": "Insights",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": true
        },
        "id": "a51931df-3ced-4e0a-90bb-563a67bccc8a"
      },
      "source": [
        "**Problem-2.07:** Which dataset you have chosen for Problem-2.6. Expalin briefly about the task in the dataset. How does your test error changes with training datasize, explore and report quantitative analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have chosen the Boston Housing Dataset from OpenML. It has 13 numerical features that describe various characteristics of Boston.\n",
        "\n",
        "CRIM: per capita crime rate\n",
        "\n",
        "ZN: proportion of residential land zoned for large lots\n",
        "\n",
        "INDUS: proportion of non-retail business acres per town\n",
        "\n",
        "CHAS: Charles River dummy variable (1 if the tract bounds river, 0 otherwise)\n",
        "\n",
        "NOX: nitric oxide concentration\n",
        "\n",
        "RM: average number of rooms per dwelling\n",
        "\n",
        "AGE: proportion of owner-occupied units built before 1940\n",
        "\n",
        "DIS: weighted distances to employment centers\n",
        "\n",
        "RAD: Index of Accessibility to Radial Highways\n",
        "\n",
        "TAX: property tax rate\n",
        "\n",
        "PTR ratio: Pupil-Teacher Ratio\n",
        "\n",
        "B: Proportion of Black Population\n",
        "\n",
        "LSTAT: percentage of lower status population\n",
        "\n",
        "Target variable: MEDV (median value of owner-occupied homes in $1000s)\n",
        "\n",
        "My main task is to predict the median house price (MEDV)\n",
        "\n",
        "The test MSE decreases as the training data size increases.\n",
        "\n",
        "![Screenshot 2025-02-06 105857.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "nBZM9qojniwh"
      },
      "id": "nBZM9qojniwh"
    },
    {
      "cell_type": "markdown",
      "id": "9ea863d5-dba1-4472-8765-0318765ad746",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "dd652301fde70fe42609457b34852ad8",
          "grade": false,
          "grade_id": "ethics",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": true
        },
        "id": "9ea863d5-dba1-4472-8765-0318765ad746"
      },
      "source": [
        "**Problem-2.08:** Ethical Reflection and Acknowledgments (Mandatory Question) (i) List all collaborators, references, or resources you used. If none, write \"NA.\"\n",
        "\n",
        "(ii) Estimate the percentage of the code you wrote yourself.\n",
        "\n",
        "(iii) Reflect on your ethical practices (Yes/No):\n",
        "\n",
        "(a) Did you avoid copying code without understanding it?\n",
        "(b) Did you properly cite all resources and collaborators?\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(i) W3Schools , GeekforGeeks , OpenML , Google(for fullforms of numerical features)\n",
        "\n",
        "(ii) 70% , I needed to take help from these resources for my codes as i dont know all the functions in ML.\n",
        "\n",
        "(iii) (a) Yes  (b) Yes"
      ],
      "metadata": {
        "id": "I-Sz5VLYvCRo"
      },
      "id": "I-Sz5VLYvCRo"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}