{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "284bfca0",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel $\\rightarrow$ Restart) and then **run all cells** (in the menubar, select Cell $\\rightarrow$ Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0f0b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c4ce80",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3178d78-d1fa-4777-ab6c-08ea2ea58489",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d088c829be3c25c943ba1d038c0183ee",
     "grade": false,
     "grade_id": "cell-9fff79d8d885a2af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## CSL2050: Pattern Recognition and Machine Learning<br>\n",
    "Programming Assignemnt-3<br>\n",
    "Spring 2025<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60778af9-fbd0-4e1c-bb1e-200ab916e4d2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "02d9be5c73c61db383e5e318e5a52987",
     "grade": false,
     "grade_id": "Q1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Q1. Please take this **Honesty Pledge** by reading and copying it and writing your full name in capital letters: \n",
    "\n",
    "``I affirm that this assignment is solely my work. I have not used unauthorised assistance, engaged in plagiarism, or violated ethical standards, to the best of my knowledge. Further, all references used and any discussion with anyone have been appropriately cited. Any breach may lead to disciplinary actions as per the course academic honesty policy discussed in Lecture-1.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c530ce94-9516-4aeb-a939-0a00ef5ea42e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1e3048bf20a9d7f1ac38122f5518b092",
     "grade": false,
     "grade_id": "cell-bf44fffef69df8a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this assignment, we shall implement a binary decision tree from scratch. The Input/Output for decision tree construction is given here:\n",
    "**Input:**\n",
    "Data: Feature matrix (shape: ùëõ√óùëö, where ùëõ is the number of samples, ùëö is the number of features).\n",
    "\n",
    "labels: Target labels (shape: ùëõ√ó1).\n",
    "\n",
    "max_depth: Maximum depth of the tree.\n",
    "\n",
    "**Output:**\n",
    "A trained decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964747ea-a75c-4c4b-ae36-139fddddd77d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2acec0878e27c288206e7220a01db824",
     "grade": false,
     "grade_id": "Q2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Q2. **Step-1: Compute entropy based on category-wise distribution of data**\n",
    "\n",
    "\\\\( H(Y) = -\\sum_{i=1}^{C} p_i log_2(p_i) \\\\)\n",
    "\n",
    "where C is the number of categories and \\\\( p_i \\\\) is probability of ith category.\n",
    "\n",
    "We will implement a function entropy that takes the labels of all data samples as input. The function will first compute the probability \\\\( p_i \\\\) each unique category in the labels. Then, using these probabilities, it will calculate and return the entropy.\n",
    "\n",
    "# Example usage\n",
    "labels = ['A', 'A', 'B', 'B']  #labels can be 0,1,2 or any non numeric values\n",
    "print(entropy(labels))  # will print 1    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e76926-4faf-4a57-a5d6-ce3c1d09d365",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "be1fc23d9b7221c880d58e89cd1d0e42",
     "grade": false,
     "grade_id": "Q2ans",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy(labels):\n",
    "    \"\"\"Compute entropy of the data with labels.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd96421b-4f56-4842-a8bb-bacd6c51aba7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ef2b2ba43bed753e2133ac9fb2205e9",
     "grade": true,
     "grade_id": "Q2test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# AUTOGRADER TEST CASE - VISIBLE\n",
    "\n",
    "assert entropy(np.array([1, 1, 1, 1])) == 0.0, \"Test case 1 failed: Expected entropy 0.0\"\n",
    "assert np.isclose(entropy(np.array([0, 0, 1, 1])), 1.0), \"Test case 2 failed: Expected entropy 1.0\"\n",
    "assert np.isclose(entropy(np.array([0, 0, 0, 1])), -(0.75 * np.log2(0.75) + 0.25 * np.log2(0.25))), \"Test case 3 failed\"\n",
    "print(\"‚úÖ All visible test cases passed!\")\n",
    "\n",
    "# AUTOGRADER TEST CASE - HIDDEN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a750d7c9-c1f7-4761-ab70-7b753c5425bb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "216e05f90b872a795b4377d961febfa2",
     "grade": false,
     "grade_id": "Q3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Q3. **Step 2: Compute Information Gain**\n",
    "\n",
    "Implement a function compute_information_gain that takes (i) data -- Feature matrix (shape: ùëõ√óùëö, where ùëõ is the number of samples, ùëö is the number of features), (ii) feature_index -- an integer that is used to select a feature to evaluate its suitability to split the dataset, and (iii) the list of class labels, and it returns info_gain and threshold if that particular feature_index is chosen.\n",
    "\n",
    "Note that here you are supposed to compute information gain based on gain in entropy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c23b374-3f11-435d-a982-0996ca7021ac",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9abed8c83bb1f03db5726ca9cd45d92c",
     "grade": false,
     "grade_id": "Q3ans",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_information_gain(data, feature_index, labels):\n",
    "    \"\"\"\n",
    "    Computes information gain of a feature for a given dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    data (np.array): Feature matrix of shape (n_samples, n_features)\n",
    "    feature_index (int): Index of the feature to compute IG\n",
    "    labels (list or np.array): List of class labels\n",
    "    \n",
    "    Returns:\n",
    "    float: Information gain\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90fc59e-79cc-47eb-a3a5-f31437d7e23d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f6733577562ed05529c72f3d804fd20",
     "grade": true,
     "grade_id": "Q3test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "data = np.array([[1, 0], [1, 1], [0, 1], [0, 0]])\n",
    "labels = np.array([0, 1, 1, 0])\n",
    "info_gain, threshold=compute_information_gain(data, 0, labels)\n",
    "assert np.isclose(info_gain, 0.0, atol=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85518ab-b7fe-418e-a2a3-eb67d35c0750",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c6c309b26de79a232352bd4463764fea",
     "grade": false,
     "grade_id": "Q4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Q4. **Step 3: Find the Best Feature and Threshold**\n",
    "\n",
    "The best feature is the one that gives maximum information gain. Use every feature and find out the one that gives the best split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e026286-306b-424b-9b36-b089a1c771ea",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "180b05278de1b0262a32c46ff6a4fe9d",
     "grade": true,
     "grade_id": "Q4ans",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def best_split(data, labels):\n",
    "    \"\"\"Find the best feature and threshold for splitting.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca33bcc1-fd44-4194-9598-0e4f9c00693a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf4a11baa5b1a5040e8a62eed8bfd8d8",
     "grade": false,
     "grade_id": "Q5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Q5. **Step 4: Recursive Tree Building**\n",
    "\n",
    "Now, we will build the decision tree. We are providing code for this segment. Your task is to understand each line of this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc7b6ec-2867-4f31-a282-a952659c1f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeNode:\n",
    "    \"\"\"Class for a decision tree node.\"\"\"\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=3):\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, data, labels, depth=0):\n",
    "        \"\"\"Recursively builds the decision tree.\"\"\"\n",
    "        # Base case: Stop splitting if pure or max depth reached\n",
    "        if depth == self.max_depth or len(np.unique(labels)) == 1: #Why?\n",
    "            return DecisionTreeNode(value=np.bincount(labels).argmax()) #Why? \n",
    "\n",
    "        # Find best feature and threshold\n",
    "        best_feature, best_threshold = best_split(data, labels)\n",
    "\n",
    "        # Partition data\n",
    "        left_mask = data[:, best_feature] <= best_threshold\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        if np.all(left_mask) or np.all(right_mask):  # Prevent infinite splitting\n",
    "            return DecisionTreeNode(value=np.bincount(labels).argmax())\n",
    "\n",
    "        left_child = self.fit(data[left_mask], labels[left_mask], depth + 1)\n",
    "        right_child = self.fit(data[right_mask], labels[right_mask], depth + 1)\n",
    "\n",
    "        return DecisionTreeNode(feature=best_feature, threshold=best_threshold, left=left_child, right=right_child)\n",
    "\n",
    "    def train(self, data, labels):\n",
    "        \"\"\"Initialize tree training.\"\"\"\n",
    "        self.root = self.fit(data, labels, depth=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db42741c-db9e-4cc8-9ac0-5783136c820e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "84c365b34c280b67206410db88baefd1",
     "grade": false,
     "grade_id": "Q6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Q6. Next, please review and understand the following code that is used to predict the class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c23a031-4ad9-48c6-995c-79b4bd6b84b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sample(node, sample):\n",
    "    \"\"\"Recursively predict the label for a single sample.\"\"\"\n",
    "    if node.value is not None:\n",
    "        return node.value\n",
    "    if sample[node.feature] <= node.threshold:\n",
    "        return predict_sample(node.left, sample)\n",
    "    else:\n",
    "        return predict_sample(node.right, sample)\n",
    "\n",
    "def predict(tree, data):\n",
    "    \"\"\"Predict labels for multiple samples.\"\"\"\n",
    "    return np.array([predict_sample(tree.root, sample) for sample in data])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c7f9bd-05bc-4abe-8669-b4019d1e02c3",
   "metadata": {},
   "source": [
    "Q7. Now, we provide a code that generates a synthetic dataset of two features, and fits the decision tree implemented above and computes accuracy. You are supposed to complete some tasks given in the below code segment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019249ff-56ea-41bb-90c5-76eb2d2cef57",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6e220856d5093aba2ead2e78659d168d",
     "grade": true,
     "grade_id": "Q7ans",
     "locked": false,
     "points": 8,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "!pip install scikit-learn\n",
    "# Generate synthetic dataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure n_informative + n_redundant <= n_features\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_informative=2, \n",
    "                           n_redundant=0, n_classes=2, random_state=42)\n",
    "print(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_test.shape)  # Check dataset shape\n",
    "\n",
    "# Train the decision tree\n",
    "tree = DecisionTree(max_depth=5)\n",
    "tree.train(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = predict(tree, X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "\n",
    "## Task-0: Write code to have 20% of train data to be used as validation set\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "## Task-1: Write code to compute accuracy on train set\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "\n",
    "## Task-2: Write code to compute accuracy on val set\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "\n",
    "## Task-3: Change max_depth from 0 to 30 and note train/val/test accuracy for each depth and plot them\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7210eb79-0565-484c-adad-88a35f7188c3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e427703438fd054eecbb193e73b53777",
     "grade": false,
     "grade_id": "Q8",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Q8. In your implementation of compute_information_gain, how was the threshold for numerical features chosen? Point out to your code where this is done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fb7df9-c128-4df5-809a-d407b0b5073a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "57f9baf40c0af05eb1f6ecc9534df981",
     "grade": false,
     "grade_id": "Q9",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Q9. What happens when the decision tree is allowed to grow too deep? Based on your observation for this assignment, write the response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0504ee-8901-4e30-975e-19e8d91d56c3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "069f5d382fe63bd006bb8d0525d50994",
     "grade": false,
     "grade_id": "Q10",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Q10. How would you prevent overfitting in your implementation? Suggest two different methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47e42d0-f464-43c4-af34-83caab594216",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "35baa84b7e647ed753a4d41da24e2fca",
     "grade": false,
     "grade_id": "Q11",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Q11. What other design choices could you have chosen for implementing the decision tree? List them out. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158a1a15-b383-4978-8b2c-89e13833f775",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6bcc6f05c4f95d99cd7d76a6ff658bb3",
     "grade": false,
     "grade_id": "Q12",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Q12. Cite all the web sources or help from individuals you have taken to complete this assignment. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
